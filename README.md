![ (c) Can Stock Photo www.canstockphoto.com / coraMax.  Used with permission. ](canstockphoto8149776.jpg)

# Multi-talker methods in Speech Processing
## Special sesssion at [Interspeech 2023](https://www.interspeech2023.org) 

Developing methods that are able to handle multiple simultaneous speakers represents a major challenge for researchers in many fields of speech technology and speech science, for example, in speech enhancement, auditory modelling and machine listening or speaking.  Significant research activity has occurred in many of these fields in recent years and great advances have been made, but often in a siloed manner.    

This cross-disciplinary special session will bring together researchers from across the whole field to present and discuss their latest research on multi-talker methods, encouraging a sharing of ideas and fertilising future collaboration.

There are a great many unsolved research questions in the area that we believe will be best solved by fostering such new relationships.  To give just a few examples:
* Should we use the same speech enhancement technqiues for ASR and human listeners?
* Could knowledge of specific speakers be embedded in future hearing aids?
* Could text-to-speech be optimised to boost intelligibility or reduce cognitive load when used in multi-talker situations?
* What gaps remain between human and machine listening in a cocktail party situation?
* How important is lip-reading and location awareness in computational auditory scene understanding?
* How can we most effectively evaluate enhancement methods considering human perception and recent advances on automatic intelligibility/quality prediction?
This session wonâ€™t solve these problems, but might bring together researchers who can.   

We welcome submissions on many different topics, including, but not limited to:
* Single channel speech separation;
* Automatic speech recognition of overlapped speech;
* Speech enhancement in the presence of competing speakers;
* Diarization of overlapped speech;
* Target speaker ASR and speech enhancement;
* Understanding human speech perception in multi-talker environments;
* Improving speech synthesis in competing-speaker scenarios;
* Multi-modal approaches to multi-talker speech processing: for example audio-visual methods, location-aware approaches;
* Clinical applications of multi-talker methods, eg. for hearing impaired listeners;
* Downstream technologies operating in multi-talker scenarios, eg. meeting transcription, human-robot interaction;
* Evaluation methods for multi-talker speech technologies.
Note however that we intend the focus of the session to be on applications in single-channel or binaural conditions, rather than on methods pertaining specifically to microphone arrays or other specialist hardware. 

We anticipate that after a short introductory talk, the session will be entirely poster-based to give maximum opportunity for cross-disciplinary networking.

### Paper submission

Papers submitted to the session should follow the regular Interspeech paper guidelines, [submission](https://www.interspeech2023.org/paper-submission/) and review process.  Accepted papers will appear in the main proceedings and the ISCA archive.  Be sure to list "Multi-talker methods in speech processing" as your paper subject area when making a submission. 

Papers must be submitted by 1 March 2023, but updates are permitted up until 8 March 2023.

### Organising team

Peter Bell, University of Edinburgh, UK   
Michael Akeroyd, University of Nottingham, UK  
Marc Delcroix, NTT, Japan  
Liang Lu, Otter.ai, USA  
Jonathan Le Roux, MERL, USA  
Jinyu Li, Microsoft, USA  
Cassia Valentini, University of Edinburgh, UK  
DeLiang Wang, Ohio State University, USA  
